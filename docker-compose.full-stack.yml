# Full Stack Docker Compose - All services containerized
version: '3.8'

services:
  # Main application container
  ai-knowledge-manager:
    build: 
      context: .
      dockerfile: Dockerfile
    container_name: ai-knowledge-manager
    ports:
      - "8080:7040"  # Access at http://localhost:8080
    volumes:
      - ai-knowledge-data:/app/data          # Persistent data storage
      - ./logs:/app/data/logs               # Mount logs for development
    environment:
      - ASPNETCORE_ENVIRONMENT=Production
      - DOTNET_RUNNING_IN_CONTAINER=true
      - VectorStore__Provider=Qdrant        # Use Qdrant
      - VectorStore__Qdrant__Host=qdrant    # Reference Qdrant service
      - VectorStore__Qdrant__Port=6334      # gRPC port
      # API keys - set these via .env file or environment
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - GEMINI_API_KEY=${GEMINI_API_KEY}
    depends_on:
      qdrant:
        condition: service_healthy
      ollama:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7040/api/ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped
    networks:
      - ai-services

  # Qdrant vector database
  qdrant:
    image: qdrant/qdrant:latest
    container_name: qdrant
    ports:
      - "6333:6333"  # REST API
      - "6334:6334"  # gRPC API
    volumes:
      - qdrant-data:/qdrant/storage
    environment:
      - QDRANT__SERVICE__HTTP_PORT=6333
      - QDRANT__SERVICE__GRPC_PORT=6334
      - QDRANT__LOG_LEVEL=INFO
    networks:
      - ai-services
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6333/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped

  # Ollama local LLM service
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"  # Ollama API
    volumes:
      - ollama-data:/root/.ollama           # Model storage
    environment:
      - OLLAMA_ORIGINS=*                    # Allow cross-origin requests
    networks:
      - ai-services
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/version"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s  # Longer start time for model loading
    restart: unless-stopped
    # Optional: GPU support
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

volumes:
  ai-knowledge-data:
    driver: local
    name: ai-knowledge-data
  qdrant-data:
    driver: local
    name: qdrant-data
  ollama-data:
    driver: local
    name: ollama-data

networks:
  ai-services:
    driver: bridge
    name: ai-services-network