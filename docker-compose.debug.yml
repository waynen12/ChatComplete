# Debug version using local build to test container issues
version: '3.8'

services:
  # AI Knowledge Manager built locally for debugging
  ai-knowledge-manager:
    build: 
      context: .
      dockerfile: Dockerfile
    container_name: ai-knowledge-manager-debug
    ports:
      - "8080:7040"  # Access at http://localhost:8080
    volumes:
      - ai-knowledge-data:/app/data          # Persistent data storage
      - ./logs:/app/logs                     # Mount logs for debugging
    environment:
      - ASPNETCORE_ENVIRONMENT=Production
      - DOTNET_RUNNING_IN_CONTAINER=true
      - ChatCompleteSettings__VectorStore__Provider=Qdrant        # Use Qdrant
      - ChatCompleteSettings__VectorStore__Qdrant__Host=qdrant    # Reference Qdrant service
      - ChatCompleteSettings__VectorStore__Qdrant__Port=6334      # gRPC port
      - ChatCompleteSettings__OllamaBaseUrl=http://ollama:11434   # Use Ollama service name
      - Logging__LogLevel__Default=Debug    # Enable debug logging
      # API keys - set these via .env file or command line
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
      - GEMINI_API_KEY=${GEMINI_API_KEY:-}
    depends_on:
      qdrant:
        condition: service_healthy
      ollama:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7040/api/ping"]
      interval: 30s
      timeout: 10s
      retries: 5  # More retries for debugging
      start_period: 120s  # Longer start period
    restart: "no"  # Don't auto-restart for debugging
    networks:
      - ai-services

  # Qdrant vector database
  qdrant:
    image: qdrant/qdrant:latest
    container_name: qdrant-debug
    ports:
      - "6333:6333"  # REST API
      - "6334:6334"  # gRPC API
    volumes:
      - qdrant-data-debug:/qdrant/storage
    environment:
      - QDRANT__SERVICE__HTTP_PORT=6333
      - QDRANT__SERVICE__GRPC_PORT=6334
      - QDRANT__LOG_LEVEL=DEBUG
    networks:
      - ai-services
    healthcheck:
      test: ["CMD-SHELL", "timeout 5s bash -c '</dev/tcp/localhost/6333' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped

  # Ollama local LLM service
  ollama:
    image: ollama/ollama:latest
    container_name: ollama-debug
    ports:
      - "11434:11434"  # Ollama API
    volumes:
      - ollama-data-debug:/root/.ollama     # Model storage
    environment:
      - OLLAMA_ORIGINS=*                    # Allow cross-origin requests
      - OLLAMA_DEBUG=1                      # Enable debug logging
    networks:
      - ai-services
    healthcheck:
      test: ["CMD-SHELL", "timeout 5s bash -c '</dev/tcp/localhost/11434' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s  # Longer start time for model loading
    restart: unless-stopped

volumes:
  ai-knowledge-data:
    driver: local
    name: ai-knowledge-data-debug
  qdrant-data-debug:
    driver: local
    name: qdrant-data-debug
  ollama-data-debug:
    driver: local
    name: ollama-data-debug

networks:
  ai-services:
    driver: bridge
    name: ai-services-network-debug