# Docker Compose using Docker Hub images
# No need to clone repository - just download this file!
version: '3.8'

services:
  # AI Knowledge Manager from Docker Hub
  ai-knowledge-manager:
    image: waynen12/ai-knowledge-manager:latest
    container_name: ai-knowledge-manager
    ports:
      - "8080:7040"  # Access at http://localhost:8080
    volumes:
      - ai-knowledge-data:/app/data          # Persistent data storage
    environment:
      - ASPNETCORE_ENVIRONMENT=Production
      - DOTNET_RUNNING_IN_CONTAINER=true
      - ChatCompleteSettings__VectorStore__Provider=Qdrant        # Use Qdrant
      - ChatCompleteSettings__VectorStore__Qdrant__Host=qdrant    # Reference Qdrant service
      - ChatCompleteSettings__VectorStore__Qdrant__Port=6334      # gRPC port for data operations
      - ChatCompleteSettings__OllamaBaseUrl=http://ollama:11434   # Use Ollama service name
      # API keys - set these via .env file or command line
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
      - GEMINI_API_KEY=${GEMINI_API_KEY:-}
    depends_on:
      qdrant:
        condition: service_healthy
      ollama:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "timeout 5s bash -c '</dev/tcp/localhost/7040' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped
    networks:
      - ai-services

  # Qdrant vector database
  qdrant:
    image: qdrant/qdrant:latest
    container_name: qdrant
    ports:
      - "6333:6333"  # REST API (optional external access)
      - "6334:6334"  # gRPC API (optional external access)
    volumes:
      - qdrant-data:/qdrant/storage
    environment:
      - QDRANT__SERVICE__HTTP_PORT=6333
      - QDRANT__SERVICE__GRPC_PORT=6334
      - QDRANT__LOG_LEVEL=INFO
    networks:
      - ai-services
    healthcheck:
      test: ["CMD-SHELL", "timeout 5s bash -c '</dev/tcp/localhost/6333' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped

  # Ollama local LLM service
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"  # Ollama API (optional external access)
    volumes:
      - ollama-data:/root/.ollama           # Model storage
    environment:
      - OLLAMA_ORIGINS=*                    # Allow cross-origin requests
    networks:
      - ai-services
    healthcheck:
      test: ["CMD-SHELL", "timeout 5s bash -c '</dev/tcp/localhost/11434' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s  # Longer start time for model loading
    restart: unless-stopped
    # Uncomment for GPU support:
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

volumes:
  ai-knowledge-data:
    driver: local
    name: ai-knowledge-data
  qdrant-data:
    driver: local
    name: qdrant-data
  ollama-data:
    driver: local
    name: ollama-data

networks:
  ai-services:
    driver: bridge
    name: ai-services-network